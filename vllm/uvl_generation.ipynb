{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/rodri/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/preprocessor_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-28 12:05:45 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-28 12:05:45 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 11-28 12:05:45 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-28 12:05:45 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-28 12:05:45 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-28 12:05:45 selector.py:135] Using Flash Attention backend.\n",
      "INFO 11-28 12:05:46 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 11-28 12:05:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/rodri/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/refs/main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# llama-3-70 quantized\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#llm = LLM('study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8', gpu_memory_utilization=0.9, tensor_parallel_size=8, enforce_eager=False, quantization=\"gptq\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#llm = LLM('meta-llama/Llama-3.2-1B-Instruct' enforce_eager=False, dtype=\"half\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-3B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py:1028\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1025\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         )\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:210\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:585\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    583\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:347\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    345\u001b[0m     model_config)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py:36\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:40\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py:152\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py:1074\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1072\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1077\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:12\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     11\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:334\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[1;32m    332\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config)\n\u001b[0;32m--> 334\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m    337\u001b[0m     quant_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:586\u001b[0m, in \u001b[0;36mLlamaForCausalLM.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]):\n\u001b[1;32m    581\u001b[0m     loader \u001b[38;5;241m=\u001b[39m AutoWeightsLoader(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m         skip_prefixes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    584\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    585\u001b[0m     )\n\u001b[0;32m--> 586\u001b[0m     \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_remap_mistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:229\u001b[0m, in \u001b[0;36mAutoWeightsLoader.load_weights\u001b[0;34m(self, weights, mapper)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     weights \u001b[38;5;241m=\u001b[39m mapper\u001b[38;5;241m.\u001b[39mapply(weights)\n\u001b[0;32m--> 229\u001b[0m autoloaded_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autoloaded_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:181\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    178\u001b[0m child_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(module\u001b[38;5;241m.\u001b[39mnamed_children())\n\u001b[1;32m    179\u001b[0m child_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(module\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_prefix, child_weights \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_groupby_prefix(weights):\n\u001b[1;32m    182\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_qualname(base_prefix, child_prefix)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child_prefix \u001b[38;5;129;01min\u001b[39;00m child_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:103\u001b[0m, in \u001b[0;36mAutoWeightsLoader._groupby_prefix\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_groupby_prefix\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]]]:\n\u001b[1;32m    100\u001b[0m     weights_by_parts \u001b[38;5;241m=\u001b[39m ((weight_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m), weight_data)\n\u001b[1;32m    101\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m weight_name, weight_data \u001b[38;5;129;01min\u001b[39;00m weights)\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prefix, group \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mgroupby(weights_by_parts,\n\u001b[1;32m    104\u001b[0m                                            key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m             prefix,\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;66;03m# Because maxsplit=1 in weight_name.split(...),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m parts, weights_data \u001b[38;5;129;01min\u001b[39;00m group),\n\u001b[1;32m    111\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:100\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_groupby_prefix\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]]]:\n\u001b[0;32m--> 100\u001b[0m     weights_by_parts \u001b[38;5;241m=\u001b[39m ((weight_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m), weight_data)\n\u001b[1;32m    101\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m weight_name, weight_data \u001b[38;5;129;01min\u001b[39;00m weights)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prefix, group \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mgroupby(weights_by_parts,\n\u001b[1;32m    104\u001b[0m                                            key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m             prefix,\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;66;03m# Because maxsplit=1 in weight_name.split(...),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m parts, weights_data \u001b[38;5;129;01min\u001b[39;00m group),\n\u001b[1;32m    111\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:586\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]):\n\u001b[1;32m    581\u001b[0m     loader \u001b[38;5;241m=\u001b[39m AutoWeightsLoader(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m         skip_prefixes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    584\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    585\u001b[0m     )\n\u001b[0;32m--> 586\u001b[0m     loader\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_remap_mistral(name, loaded_weight)\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, loaded_weight \u001b[38;5;129;01min\u001b[39;00m weights)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:313\u001b[0m, in \u001b[0;36mDefaultModelLoader._get_all_weights\u001b[0;34m(self, model_config, model)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_all_weights\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     model_config: ModelConfig,\n\u001b[1;32m    304\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    307\u001b[0m     primary_weights \u001b[38;5;241m=\u001b[39m DefaultModelLoader\u001b[38;5;241m.\u001b[39mSource(\n\u001b[1;32m    308\u001b[0m         model_config\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    309\u001b[0m         model_config\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    310\u001b[0m         prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    311\u001b[0m         fall_back_to_pt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall_back_to_pt_during_load\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    312\u001b[0m                                 \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_weights_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimary_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     secondary_weights \u001b[38;5;241m=\u001b[39m cast(Iterable[DefaultModelLoader\u001b[38;5;241m.\u001b[39mSource],\n\u001b[1;32m    316\u001b[0m                              \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecondary_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, ()))\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m secondary_weights:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:272\u001b[0m, in \u001b[0;36mDefaultModelLoader._get_weights_iterator\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_weights_iterator\u001b[39m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28mself\u001b[39m, source: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get an iterator for the model weights based on the load format.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     hf_folder, hf_weights_files, use_safetensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m==\u001b[39m LoadFormat\u001b[38;5;241m.\u001b[39mNPCACHE:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# Currently np_cache only support *.bin checkpoints\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m use_safetensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:228\u001b[0m, in \u001b[0;36mDefaultModelLoader._prepare_weights\u001b[0;34m(self, model_name_or_path, revision, fall_back_to_pt)\u001b[0m\n\u001b[1;32m    225\u001b[0m     allow_patterns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[0;32m--> 228\u001b[0m     hf_folder \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_weights_from_hf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     hf_folder \u001b[38;5;241m=\u001b[39m model_name_or_path\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/weight_utils.py:247\u001b[0m, in \u001b[0;36mdownload_weights_from_hf\u001b[0;34m(model_name_or_path, cache_dir, allow_patterns, revision, ignore_patterns)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Use file lock to prevent multiple processes from\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# downloading the same model weights at the same time.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_lock(model_name_or_path, cache_dir):\n\u001b[0;32m--> 247\u001b[0m     hf_folder \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDisabledTqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_OFFLINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hf_folder\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py:260\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    258\u001b[0m     ref_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(storage_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefs\u001b[39m\u001b[38;5;124m\"\u001b[39m, revision)\n\u001b[1;32m    259\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(ref_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mref_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    261\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(commit_hash)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# we pass the commit_hash to hf_hub_download\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# so no network call happens if we already\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# have the file locally.\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/rodri/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/refs/main'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# llama-3-70 quantized\n",
    "#llm = LLM('study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8', gpu_memory_utilization=0.9, tensor_parallel_size=8, enforce_eager=False, quantization=\"gptq\")\n",
    "#llm = LLM('meta-llama/Llama-3.2-1B-Instruct' enforce_eager=False, dtype=\"half\")\n",
    "llm = LLM(\"meta-llama/Llama-3.2-3B-Instruct\", enforce_eager=False, dtype=\"half\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 9593\n",
      "rog0d: First step -> Exclusive guided decodings\n",
      "rog0d: Second step -> validate and add request\n",
      "rog0d: third step-> Decoding backend\n",
      "rog0d: fourth step-> outlines backend\n",
      "rog0d: fifth step-> outlines operation\n",
      "rog0d: sixth step-> check outlines guiding options\n",
      "rog0d: seventh step-> CFGLogitsProcessor class: CFGuide to BaseLogitsProcessor\n",
      "rog0d: ninth step-> _init_ CFGGuide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rog0d: eighth step-> _call_ BaseLogitsProcessor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 11/11 [00:00<00:00, 17.95it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 3/3 [00:00<00:00,  8.16it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 4/4 [00:00<00:00, 10.01it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 3/3 [00:00<00:00,  5.67it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 6/6 [00:00<00:00,  9.75it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 28/28 [00:01<00:00, 24.63it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 4/4 [00:00<00:00, 10.05it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 29/29 [00:01<00:00, 24.78it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 5/5 [00:00<00:00,  8.50it/s]\n",
      "Compiling FSM index for all state transitions:  50%|█████     | 1/2 [00:00<00:00,  3.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:56<00:00, 56.50s/it, est. speed input: 3.68 toks/s, output: 3.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for inference: 57.09211522806436 seconds\n",
      "Grammar is well-written.\n",
      "Generated text without parser: 'features\\n->Feature1\\n   ->mandatory\\n       ->FeatureA\\n       ->optional\\n           ->FeatureB\\n           ->optional\\n               ->FeatureC\\n<-<-<-<-<-<-<-               \\t\\t   \\t\\t\\t        \\t      \\t            \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t\\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t    \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t    \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t'\n",
      "featuremodel\n",
      "  feature\n",
      "    Feature1\n",
      "    mandatory_group\n",
      "      groupspec\n",
      "        feature\n",
      "          FeatureA\n",
      "          optional_group\n",
      "            groupspec\n",
      "              feature\n",
      "                FeatureB\n",
      "                optional_group\n",
      "                  groupspec\n",
      "                    feature\tFeatureC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_seed = random.randint(0, 10000)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f\"seed: {random_seed}\")\n",
    "\n",
    "uvl_grammar_no_indent = r\"\"\"\n",
    "?start: _NL* featuremodel\n",
    "\n",
    "featuremodel: \"features\" _NL [feature+]\n",
    "feature: NAME _NL (group+)?\n",
    "\n",
    "group: \"or\" groupspec          -> or_group\n",
    "    | \"alternative\" groupspec -> alternative_group\n",
    "    | \"optional\" groupspec    -> optional_group\n",
    "    | \"mandatory\" groupspec   -> mandatory_group\n",
    "    | cardinality groupspec    -> cardinality_group\n",
    "\n",
    "groupspec: _NL feature+\n",
    "\n",
    "cardinality: \"[\" INT (\"..\" (INT | \"*\"))? \"]\"\n",
    "\n",
    "%import common.INT\n",
    "%import common.CNAME -> NAME\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "\n",
    "_NL: /\\r?\\n[ \\t]*/\n",
    "\"\"\"\n",
    "\n",
    "uvl_grammar = r\"\"\"\n",
    "?start: _NL* featuremodel\n",
    "\n",
    "featuremodel: \"features\" _NL [\"->\" feature+ \"<-\"]\n",
    "feature: NAME _NL (\"->\" group+ \"<-\")?\n",
    "\n",
    "group: \"or\" groupspec          -> or_group\n",
    "    | \"alternative\" groupspec -> alternative_group\n",
    "    | \"optional\" groupspec    -> optional_group\n",
    "    | \"mandatory\" groupspec   -> mandatory_group\n",
    "    | cardinality groupspec    -> cardinality_group\n",
    "\n",
    "groupspec: _NL \"->\" feature+ \"<-\"\n",
    "\n",
    "cardinality: \"[\" INT (\"..\" (INT | \"*\"))? \"]\"\n",
    "\n",
    "%import common.INT\n",
    "%import common.CNAME -> NAME\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "\n",
    "_NL: /\\r?\\n[ \\t]*/\n",
    "\"\"\"\n",
    "\n",
    "uvl_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for creating sintactically correct expressions similar to these examples:\n",
    "\n",
    "features\n",
    "    Sandwich\n",
    "        mandatory\n",
    "            Bread\n",
    "        optional\n",
    "            Sauce\n",
    "                alternative\n",
    "                    Ketchup\n",
    "                    Mustard\n",
    "            Cheese\n",
    "constraints\n",
    "    Ketchup => Cheese\n",
    ",\n",
    "features\n",
    "\tSmartWatch\n",
    "\t\tmandatory\n",
    "\t\t\tFunctionalities \n",
    "\t\t\t\tmandatory\n",
    "\t\t\t\t\tFitnessMonitor\n",
    "\t\t\t\t\tSleepTracker\n",
    "\t\t\t\t\tVibrateAlert\n",
    "\t\t\t\t\t\tmandatory\n",
    "\t\t\t\t\t\t\tCall\n",
    "\t\t\t\t\t\t\tNotification\n",
    "\t\t\tSensors\n",
    "\t\t\t\tmandatory\n",
    "\t\t\t\t\tPedometer\n",
    "\t\t\t\t\tAccelerometer\n",
    "\t\t\t\toptional\n",
    "\t\t\t\t\tHeartRateSensor\n",
    "\t\t\tConnectivity\n",
    "\t\t\t\tmandatory\n",
    "\t\t\t\t\tBT40\n",
    "\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Write new, valid expressions for a computer: \n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "grammar = uvl_grammar\n",
    "prompt = uvl_prompt\n",
    "# llama-3 8B\n",
    "#MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#llm = LLM(MODEL, gpu_memory_utilization=1, tensor_parallel_size=8, enforce_eager=False, dtype=\"half\") \n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        max_tokens=200,\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        min_tokens=200,\n",
    "    )\n",
    "\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts=prompt,\n",
    "    sampling_params=sampling_params,\n",
    "    guided_options_request=dict(guided_grammar=grammar))\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f'Elapsed time for inference: {elapsed_time} seconds')\n",
    "\n",
    "from lark import Lark, exceptions\n",
    "from lark.indenter import Indenter\n",
    "\n",
    "\n",
    "def test(generation: str, parser):\n",
    "    print(parser.parse(generation).pretty())\n",
    "\n",
    "\"\"\"\n",
    "class TreeIndenter(Indenter):\n",
    "    NL_type = '_NL'\n",
    "    OPEN_PAREN_types = []\n",
    "    CLOSE_PAREN_types = []\n",
    "    INDENT_type = '_INDENT'\n",
    "    DEDENT_type = '_DEDENT'\n",
    "    tab_len = 8\n",
    "\"\"\"\n",
    "\n",
    "#parser = Lark(grammar, parser='lalr', postlex=TreeIndenter())\n",
    "parser = Lark(grammar, parser='lalr')\n",
    "\n",
    "print(\"Grammar is well-written.\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text without parser: {generated_text!r}\")\n",
    "\n",
    "    try:\n",
    "        # Parse a generation\n",
    "        test(generation=generated_text, parser=parser)\n",
    "\n",
    "    except:\n",
    "        print(\"Generation not grammatically valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº of generated tokens: 200\n",
      "generated tokens id: array('l', [13043, 82, 198, 405, 14180, 16, 198, 262, 405, 81216, 198, 286, 405, 14180, 32, 198, 286, 405, 13099, 198, 310, 405, 14180, 33, 198, 310, 405, 13099, 198, 394, 405, 14180, 34, 198, 46442, 46442, 46442, 46442, 27, 12, 46442, 27, 12, 2342, 220, 6585, 573, 17815, 52470, 36657, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 18952, 573, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 99978, 15646, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 99978, 15646, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646])\n"
     ]
    }
   ],
   "source": [
    "print(f\"nº of generated tokens: {str(len(outputs[0].outputs[0].token_ids))}\")\n",
    "print(f\"generated tokens id: {outputs[0].outputs[0].token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "->Feature1\n",
      "   ->mandatory\n",
      "       ->FeatureA\n",
      "       ->optional\n",
      "           ->FeatureB\n",
      "           ->optional\n",
      "               ->FeatureC\n",
      "<-<-<-<-<-<-<-               \t\t   \t\t\t        \t      \t            \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t\t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t    \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t    \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\n",
      "featuremodel\n",
      "  feature\n",
      "    Feature1\n",
      "    mandatory_group\n",
      "      groupspec\n",
      "        feature\n",
      "          FeatureA\n",
      "          optional_group\n",
      "            groupspec\n",
      "              feature\n",
      "                FeatureB\n",
      "                optional_group\n",
      "                  groupspec\n",
      "                    feature\tFeatureC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen = outputs[0].outputs[0].text\n",
    "print(f\"{str(gen)}\")\n",
    "\n",
    "print(parser.parse(gen).pretty())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
