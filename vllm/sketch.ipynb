{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-05 13:57:31,825\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 13:57:35 config.py:318] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 11-05 13:57:35 config.py:813] Defaulting to use mp for distributed inference\n",
      "INFO 11-05 13:57:35 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8', speculative_config=None, tokenizer='study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 13:57:36 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-05 13:57:36 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:36 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:36 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:36 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:57:36 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:36 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:37 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:37 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:37 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:37 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:57:36 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:57:37 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m /home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/home/rog0d/miniconda3/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:39 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-05 13:57:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-05 13:57:41 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-05 13:57:41 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f3569089600>, local_subscribe_port=39485, remote_subscribe_port=None)\n",
      "INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "INFO 11-05 13:57:41 model_runner.py:879] Starting to load model study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8...\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-05 13:57:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "INFO 11-05 13:57:41 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:42 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.07it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:03,  1.28it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.19it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:57:49 model_runner.py:890] Loading model weights took 8.6492 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:49 model_runner.py:890] Loading model weights took 8.6492 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:57:50 model_runner.py:890] Loading model weights took 8.6492 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:57:50 model_runner.py:890] Loading model weights took 8.6492 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:57:50 model_runner.py:890] Loading model weights took 8.6492 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:57:50 model_runner.py:890] Loading model weights took 8.6492 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:57:51 model_runner.py:890] Loading model weights took 8.6492 GB\n",
      "INFO 11-05 13:57:51 model_runner.py:890] Loading model weights took 8.6492 GB\n",
      "INFO 11-05 13:57:55 distributed_gpu_executor.py:56] # GPU blocks: 4073, # CPU blocks: 6553\n",
      "INFO 11-05 13:57:59 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-05 13:57:59 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:59 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m INFO 11-05 13:57:59 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:58:00 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-05 13:58:00 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:58:00 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-05 13:58:00 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:58:00 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-05 13:58:00 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:58:00 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-05 13:58:00 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:58:01 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:58:01 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:58:01 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:58:01 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638689)\u001b[0;0m INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 38 secs.\n",
      "INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 41 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638690)\u001b[0;0m INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 40 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638695)\u001b[0;0m INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 40 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638688)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638710)\u001b[0;0m INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 41 secs.\n",
      "INFO 11-05 13:58:39 model_runner.py:1300] Graph capturing finished in 38 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638692)\u001b[0;0m INFO 11-05 13:58:40 model_runner.py:1300] Graph capturing finished in 40 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638691)\u001b[0;0m INFO 11-05 13:58:40 model_runner.py:1300] Graph capturing finished in 40 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# llama-3-70 quantized\n",
    "llm = LLM('study-hjt/Meta-Llama-3-70B-Instruct-GPTQ-Int8', gpu_memory_utilization=0.9, tensor_parallel_size=8, enforce_eager=False, quantization=\"gptq\")\n",
    "#llm = LLM('meta-llama/Llama-3.2-1B-Instruct', gpu_memory_utilization=0.9, tensor_parallel_size=8, enforce_eager=False, dtype=\"half\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 9593\n",
      "rog0d: First step -> Exclusive guided decodings\n",
      "rog0d: Second step -> validate and add request\n",
      "rog0d: third step-> Decoding backend\n",
      "rog0d: fourth step-> outlines backend\n",
      "rog0d: fifth step-> outlines operation\n",
      "rog0d: sixth step-> check outlines guiding options\n",
      "rog0d: seventh step-> CFGLogitsProcessor class: CFGuide to BaseLogitsProcessor\n",
      "rog0d: ninth step-> _init_ CFGGuide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rog0d: eighth step-> _call_ BaseLogitsProcessor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 11/11 [00:00<00:00, 17.95it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 3/3 [00:00<00:00,  8.16it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 4/4 [00:00<00:00, 10.01it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 3/3 [00:00<00:00,  5.67it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 6/6 [00:00<00:00,  9.75it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 28/28 [00:01<00:00, 24.63it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 4/4 [00:00<00:00, 10.05it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 29/29 [00:01<00:00, 24.78it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 5/5 [00:00<00:00,  8.50it/s]\n",
      "Compiling FSM index for all state transitions:  50%|█████     | 1/2 [00:00<00:00,  3.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:56<00:00, 56.50s/it, est. speed input: 3.68 toks/s, output: 3.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for inference: 57.09211522806436 seconds\n",
      "Grammar is well-written.\n",
      "Generated text without parser: 'features\\n->Feature1\\n   ->mandatory\\n       ->FeatureA\\n       ->optional\\n           ->FeatureB\\n           ->optional\\n               ->FeatureC\\n<-<-<-<-<-<-<-               \\t\\t   \\t\\t\\t        \\t      \\t            \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t\\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t    \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t\\t    \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t  \\t        \\t      \\t\\t    \\t      \\t        \\t\\t    \\t        \\t      \\t        \\t\\t    \\t'\n",
      "featuremodel\n",
      "  feature\n",
      "    Feature1\n",
      "    mandatory_group\n",
      "      groupspec\n",
      "        feature\n",
      "          FeatureA\n",
      "          optional_group\n",
      "            groupspec\n",
      "              feature\n",
      "                FeatureB\n",
      "                optional_group\n",
      "                  groupspec\n",
      "                    feature\tFeatureC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_seed = random.randint(0, 10000)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f\"seed: {random_seed}\")\n",
    "\n",
    "uvl_grammar_no_indent = r\"\"\"\n",
    "?start: _NL* featuremodel\n",
    "\n",
    "featuremodel: \"features\" _NL [feature+]\n",
    "feature: NAME _NL (group+)?\n",
    "\n",
    "group: \"or\" groupspec          -> or_group\n",
    "    | \"alternative\" groupspec -> alternative_group\n",
    "    | \"optional\" groupspec    -> optional_group\n",
    "    | \"mandatory\" groupspec   -> mandatory_group\n",
    "    | cardinality groupspec    -> cardinality_group\n",
    "\n",
    "groupspec: _NL feature+\n",
    "\n",
    "cardinality: \"[\" INT (\"..\" (INT | \"*\"))? \"]\"\n",
    "\n",
    "%import common.INT\n",
    "%import common.CNAME -> NAME\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "\n",
    "_NL: /\\r?\\n[ \\t]*/\n",
    "\"\"\"\n",
    "\n",
    "uvl_grammar = r\"\"\"\n",
    "?start: _NL* featuremodel\n",
    "\n",
    "featuremodel: \"features\" _NL [\"->\" feature+ \"<-\"]\n",
    "feature: NAME _NL (\"->\" group+ \"<-\")?\n",
    "\n",
    "group: \"or\" groupspec          -> or_group\n",
    "    | \"alternative\" groupspec -> alternative_group\n",
    "    | \"optional\" groupspec    -> optional_group\n",
    "    | \"mandatory\" groupspec   -> mandatory_group\n",
    "    | cardinality groupspec    -> cardinality_group\n",
    "\n",
    "groupspec: _NL \"->\" feature+ \"<-\"\n",
    "\n",
    "cardinality: \"[\" INT (\"..\" (INT | \"*\"))? \"]\"\n",
    "\n",
    "%import common.INT\n",
    "%import common.CNAME -> NAME\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "\n",
    "_NL: /\\r?\\n[ \\t]*/\n",
    "\"\"\"\n",
    "\n",
    "uvl_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for creating gramatically and sintactically expression given this specific grammar: {uvl_grammar}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Write new, valid expressions: \n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "arithmetic_grammar = \"\"\"\n",
    "?start: comparison\n",
    "\n",
    "?comparison: expression (\"=\" expression)?\n",
    "\n",
    "?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "?factor: NUMBER\n",
    "       | \"-\" factor\n",
    "       | \"(\" comparison \")\"\n",
    "\n",
    "%import common.NUMBER\n",
    "%ignore \" \"  // Ignore spaces\n",
    "\n",
    "\"\"\"\n",
    "arithmetic_prompt=\"Rewrite 5*5 as another expression\"\n",
    "\n",
    "arithmetic_prompt_fewshots=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for creating gramatically, equivalent and correct arithmetical expression<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Rewrite the following expressions into equivalent ones as show in this example(5*5)=(5+5+5+5+5)=(25*1)=(5*3)+(5*2).\\n(3*3)=\\n(3*4*5)=\\n(7*3)=\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "sql_grammar=\"\"\"\n",
    "start: select_statement\n",
    "select_statement: \"SELECT\" column \"from\" table \"where\" condition\n",
    "column: \"col_1\" | \"col_2\"\n",
    "table: \"table_1\" | \"table_2\"\n",
    "condition: column \"=\" number\n",
    "number: \"1\" | \"2\"\n",
    "\"\"\"\n",
    "sql_prompt=\"Generate a sql state that select col_1 from table_1 where it is equals to 1\"\n",
    "\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "grammar = uvl_grammar\n",
    "prompt = uvl_prompt\n",
    "# llama-3 8B\n",
    "#MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#llm = LLM(MODEL, gpu_memory_utilization=1, tensor_parallel_size=8, enforce_eager=False, dtype=\"half\") \n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        max_tokens=200,\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        min_tokens=200,\n",
    "    )\n",
    "\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts=prompt,\n",
    "    sampling_params=sampling_params,\n",
    "    guided_options_request=dict(guided_grammar=grammar))\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f'Elapsed time for inference: {elapsed_time} seconds')\n",
    "\n",
    "from lark import Lark, exceptions\n",
    "from lark.indenter import Indenter\n",
    "\n",
    "\n",
    "def test(generation: str, parser):\n",
    "    print(parser.parse(generation).pretty())\n",
    "\n",
    "\"\"\"\n",
    "class TreeIndenter(Indenter):\n",
    "    NL_type = '_NL'\n",
    "    OPEN_PAREN_types = []\n",
    "    CLOSE_PAREN_types = []\n",
    "    INDENT_type = '_INDENT'\n",
    "    DEDENT_type = '_DEDENT'\n",
    "    tab_len = 8\n",
    "\"\"\"\n",
    "\n",
    "#parser = Lark(grammar, parser='lalr', postlex=TreeIndenter())\n",
    "parser = Lark(grammar, parser='lalr')\n",
    "\n",
    "print(\"Grammar is well-written.\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text without parser: {generated_text!r}\")\n",
    "\n",
    "    try:\n",
    "        # Parse a generation\n",
    "        test(generation=generated_text, parser=parser)\n",
    "\n",
    "    except:\n",
    "        print(\"Generation not grammatically valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº of generated tokens: 200\n",
      "generated tokens id: array('l', [13043, 82, 198, 405, 14180, 16, 198, 262, 405, 81216, 198, 286, 405, 14180, 32, 198, 286, 405, 13099, 198, 310, 405, 14180, 33, 198, 310, 405, 13099, 198, 394, 405, 14180, 34, 198, 46442, 46442, 46442, 46442, 27, 12, 46442, 27, 12, 2342, 220, 6585, 573, 17815, 52470, 36657, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 18952, 573, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 99978, 15646, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 99978, 15646, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646, 19827, 17815, 87459, 7938, 52470, 38020, 7938, 17815, 996, 24311, 57646])\n"
     ]
    }
   ],
   "source": [
    "print(f\"nº of generated tokens: {str(len(outputs[0].outputs[0].token_ids))}\")\n",
    "print(f\"generated tokens id: {outputs[0].outputs[0].token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "->Feature1\n",
      "   ->mandatory\n",
      "       ->FeatureA\n",
      "       ->optional\n",
      "           ->FeatureB\n",
      "           ->optional\n",
      "               ->FeatureC\n",
      "<-<-<-<-<-<-<-               \t\t   \t\t\t        \t      \t            \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t\t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t    \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\t    \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t  \t        \t      \t\t    \t      \t        \t\t    \t        \t      \t        \t\t    \t\n",
      "featuremodel\n",
      "  feature\n",
      "    Feature1\n",
      "    mandatory_group\n",
      "      groupspec\n",
      "        feature\n",
      "          FeatureA\n",
      "          optional_group\n",
      "            groupspec\n",
      "              feature\n",
      "                FeatureB\n",
      "                optional_group\n",
      "                  groupspec\n",
      "                    feature\tFeatureC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen = outputs[0].outputs[0].text\n",
    "print(f\"{str(gen)}\")\n",
    "\n",
    "print(parser.parse(gen).pretty())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
